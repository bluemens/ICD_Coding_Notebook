{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDT6eJKKtp+xi3fQgduTDI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluemens/ICD_Coding_Notebook/blob/main/ICD_coding_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install transformers nltk --quiet"
      ],
      "metadata": {
        "id": "ls1YGIyowp1w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.tree import export_text"
      ],
      "metadata": {
        "id": "y_X9MU2Owufn"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download sentence tokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qA29eFxwygo",
        "outputId": "f7e824bd-fbd1-4699-f0eb-e00c9d278d2d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ClinicalBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "bert_model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "bert_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TTRDEV60w6z4",
        "outputId": "a703a093-cc39-4609-f6eb-2c06c9dc0600"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample raw note\n",
        "sample_note = \"\"\"\n",
        "CHIEF COMPLAINT: SOB, fatigue. Other stuff\n",
        "HPI: 65 y/o M w/ h/o CHF presents w/ SOB x 3 days, worse on exertion. Denies CP.\n",
        "ASSESSMENT: Likely CHF exacerbation. Will start IV Lasix.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zOmmifu_xVF4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_sections(note):\n",
        "    \"\"\"\n",
        "    Parse a clinical note into labeled sections using regex-based headers.\n",
        "    Returns a dict {section_name: section_text}\n",
        "    \"\"\"\n",
        "    # Common section headers (expand as needed)\n",
        "    section_headers = [\n",
        "        \"CHIEF COMPLAINT\", \"HISTORY OF PRESENT ILLNESS\", \"HPI\",\n",
        "        \"PAST MEDICAL HISTORY\", \"FAMILY HISTORY\", \"SOCIAL HISTORY\",\n",
        "        \"PHYSICAL EXAM\", \"ASSESSMENT\", \"PLAN\", \"ASSESSMENT AND PLAN\",\n",
        "        \"REVIEW OF SYSTEMS\", \"MEDICATIONS\", \"LABS\", \"DISCHARGE DIAGNOSIS\"\n",
        "    ]\n",
        "\n",
        "    # Normalize headers\n",
        "    section_pattern = \"|\".join([re.escape(h) for h in section_headers])\n",
        "    regex = re.compile(rf\"^\\s*({section_pattern})\\s*[:\\-]?\\s*$\", re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "    sections = {}\n",
        "    matches = list(regex.finditer(note))\n",
        "\n",
        "    for i, match in enumerate(matches):\n",
        "        start = match.end()\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(note)\n",
        "        section_name = match.group(1).strip().upper()\n",
        "        section_text = note[start:end].strip()\n",
        "        sections[section_name] = section_text\n",
        "\n",
        "    return sections"
      ],
      "metadata": {
        "id": "fLJimwCrjLrN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_note(note):\n",
        "    \"\"\"\n",
        "    Parses note into sections, tokenizes into sentences,\n",
        "    maps sentences to sections, prepares input for ClinicalBERT.\n",
        "    \"\"\"\n",
        "    structured = {}\n",
        "    structured[\"note_id\"] = \"example-001\"\n",
        "    structured[\"sections\"] = parse_sections(note)\n",
        "\n",
        "    sentences = []\n",
        "    sentence_to_section = {}\n",
        "\n",
        "    # Tokenize by sentence, track section\n",
        "    for section, text in structured[\"sections\"].items():\n",
        "        section_sentences = nltk.sent_tokenize(text)\n",
        "        for sent in section_sentences:\n",
        "            sentence_to_section[len(sentences)] = section\n",
        "            sentences.append(sent)\n",
        "\n",
        "    # Join all for BERT input\n",
        "    full_doc = \" \".join(sentences)\n",
        "    encoded = tokenizer(full_doc, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    structured[\"sentences\"] = sentences\n",
        "    structured[\"sentence_to_section\"] = sentence_to_section\n",
        "    structured[\"full_doc\"] = full_doc\n",
        "    structured[\"tokens\"] = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0])\n",
        "    structured[\"token_ids\"] = encoded[\"input_ids\"][0].tolist()\n",
        "    structured[\"encoded\"] = encoded  # optional: keep raw inputs for later\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = bert_model(**encoded)\n",
        "        structured[\"cls_embedding\"] = output.last_hidden_state[:, 0, :].squeeze(0)  # shape [768]\n",
        "\n",
        "\n",
        "    return structured"
      ],
      "metadata": {
        "id": "EyepVzh_w_dz"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experimental example note with Sections (structured not free)\n",
        "note = \"\"\"\n",
        "CHIEF COMPLAINT:\n",
        "Chest pain and shortness of breath.\n",
        "\n",
        "HPI:\n",
        "Patient reports SOB worsening over 3 days.\n",
        "\n",
        "ASSESSMENT:\n",
        "CHF exacerbation. Start IV Lasix.\n",
        "\"\"\"\n",
        "\n",
        "#Free Text Examples\n",
        "clinical_notes = [\n",
        "    \"Patient presents with chest pain and shortness of breath.\",\n",
        "    \"Type 2 diabetes with foot ulcer.\",\n",
        "    \"Sepsis post-surgery with fever.\",\n",
        "    \"Asthma and wheezing.\",\n",
        "    \"Appendicitis and abdominal pain.\",\n",
        "]\n",
        "\n",
        "\n",
        "data = preprocess_note(note)\n",
        "print(data[\"sentences\"])\n",
        "print(data[\"sentence_to_section\"])\n",
        "print(data[\"tokens\"][:20])  # preview\n",
        "print(data[\"token_ids\"][:20])  # preview"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUONKHa54A00",
        "outputId": "81823992-d7f1-4c86-d8ff-d4eb1d2fed05"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chest pain and shortness of breath.', 'Patient reports SOB worsening over 3 days.', 'CHF exacerbation.', 'Start IV Lasix.']\n",
            "{0: 'CHIEF COMPLAINT', 1: 'HPI', 2: 'ASSESSMENT', 3: 'ASSESSMENT'}\n",
            "['[CLS]', 'chest', 'pain', 'and', 'short', '##ness', 'of', 'breath', '.', 'patient', 'reports', 'sob', 'worse', '##ning', 'over', '3', 'days', '.', 'ch', '##f']\n",
            "[101, 2229, 2489, 1105, 1603, 1757, 1104, 2184, 119, 5351, 3756, 20295, 4146, 3381, 1166, 124, 1552, 119, 22572, 2087]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Concept Extraction\n",
        "def load_icd10_blocks_from_txt(file_path):\n",
        "    \"\"\"\n",
        "    Load ICD-10 block concept vocabulary from a structured .txt file.\n",
        "    Each line should follow the format: CODE-CODE  Description\n",
        "    Example: A00-A09  Intestinal infectious diseases\n",
        "    \"\"\"\n",
        "    block_pattern = re.compile(r'^([A-Z]\\d{2})-([A-Z]\\d{2})\\s+(.*)$')\n",
        "    blocks = []\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            match = block_pattern.match(line.strip())\n",
        "            if match:\n",
        "                start_code, end_code, description = match.groups()\n",
        "                blocks.append({\n",
        "                    \"block_start\": start_code,\n",
        "                    \"block_end\": end_code,\n",
        "                    \"concept_name\": description\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(blocks)\n",
        "\n",
        "\n",
        "def icd_code_to_block(code, block_df):\n",
        "    code = code.strip().upper()\n",
        "    for _, row in block_df.iterrows():\n",
        "        if row[\"block_start\"] <= code <= row[\"block_end\"]:\n",
        "            return row[\"concept_name\"]\n",
        "    return None\n",
        "\n",
        "# Embed all concept descriptions\n",
        "def get_concept_description_embeddings(concept_names):\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for desc in concept_names:\n",
        "            inputs = tokenizer(desc, return_tensors=\"pt\").to(device)\n",
        "            output = bert_model(**inputs)\n",
        "            cls = output.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "            embeddings.append(cls.squeeze(0))\n",
        "    return torch.stack(embeddings)  # [K, 768]\n"
      ],
      "metadata": {
        "id": "wE1cz4L1T4Ai"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_df = load_icd10_blocks_from_txt(\"/content/icd_blocks.txt\")\n",
        "NUM_CONCEPTS = block_df.shape[0]\n",
        "concept_vocab = block_df[\"concept_name\"].tolist()\n",
        "concept_index = {concept: i for i, concept in enumerate(concept_vocab)}\n",
        "\n",
        "description_embeddings = get_concept_description_embeddings(concept_vocab).to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XTNLNTVNUt95"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConceptDataset(Dataset):\n",
        "    def __init__(self, notes, label_blocks, concept_index):\n",
        "        self.notes = notes\n",
        "        self.labels = label_blocks\n",
        "        self.concept_index = concept_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.notes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cls = self.data[idx][\"cls_embedding\"]\n",
        "        c = [0] * len(self.concept_index)\n",
        "        for label in self.labels[idx]:\n",
        "            if label in self.concept_index:\n",
        "                c[self.concept_index[label]] = 1\n",
        "        return cls, torch.tensor(c).float()\n",
        "\n",
        "\n",
        "class ConceptPredictor(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, num_concepts=len(concept_names)):\n",
        "        super().__init__()\n",
        "        self.feature_net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        self.output_layer = nn.Linear(256, num_concepts)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, cls_embedding):\n",
        "        feats = self.feature_net(cls_embedding)\n",
        "        logits = self.output_layer(feats)\n",
        "        probs = self.sigmoid(logits)\n",
        "        return probs"
      ],
      "metadata": {
        "id": "NjcyMia9Ywpd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    cls_embeddings, labels = zip(*batch)\n",
        "    return torch.stack(cls_embeddings), torch.stack(labels)"
      ],
      "metadata": {
        "id": "NcyBqYO87PTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Simulated dataset ===\n",
        "note_icd_codes = [\n",
        "    [\"I50.9\", \"J18.9\"],     # Heart failure, Pneumonia\n",
        "    [\"E11.9\"],              # Type 2 diabetes\n",
        "    [\"A41.9\"]             # Sepsis\n",
        "]\n",
        "\n",
        "note_labels = []\n",
        "for code_list in note_icd_codes:\n",
        "    blocks = set()\n",
        "    for code in code_list:\n",
        "        block = icd_code_to_block(code, block_df)\n",
        "        if block:\n",
        "            blocks.add(block)\n",
        "    note_labels.append(list(blocks))"
      ],
      "metadata": {
        "id": "1ElbCUFRXhfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Setup training ===\n",
        "dataset = ConceptDataset(clinical_notes, note_labels, concept_index)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "concept_model = ConceptPredictor().to(device)\n",
        "optimizer = torch.optim.Adam(concept_model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.BCELoss()"
      ],
      "metadata": {
        "id": "FyQ36-xz7r4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    concept_model.train()\n",
        "\n",
        "    for cls_batch, label_batch in dataloader:\n",
        "        cls_batch = cls_batch.to(device)\n",
        "        label_batch = label_batch.to(device)\n",
        "\n",
        "        preds = concept_model(cls_batch)\n",
        "\n",
        "        # Standard concept prediction loss\n",
        "        L_bce = loss_fn(preds, targets)\n",
        "\n",
        "        # Alignment loss: output weights should match BERT embeddings of concepts\n",
        "        W = concept_model.output_layer.weight  # shape: [K, 256]\n",
        "        W_proj = W @ concept_model.feature_net[0].weight.T  # Project to input space\n",
        "        L_align = torch.nn.functional.mse_loss(W_proj, description_embeddings)\n",
        "\n",
        "        # Total loss (λ = 0.1)\n",
        "        loss = L_bce + 0.1 * L_align\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}: Loss = {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "FYP7VE7769Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SYMBOLIC COMPRESSION\n",
        "concept_model.eval()\n",
        "concept_vectors = []\n",
        "target_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for cls_batch, label_batch in dataloader:\n",
        "        cls_batch = cls_batch.to(device)\n",
        "        preds = concept_model(cls_batch).cpu()\n",
        "        concept_vectors.append(preds)\n",
        "        target_labels.append(label_batch)\n",
        "\n",
        "X = torch.cat(concept_vectors).numpy()  # shape: [N, K]\n",
        "Y = torch.cat(target_labels).numpy()    # shape: [N, K]"
      ],
      "metadata": {
        "id": "U_UeDh3j8L4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tree Fitting\n",
        "multi_tree = MultiOutputClassifier(\n",
        "    DecisionTreeClassifier(max_depth=4, min_samples_leaf=5)\n",
        ")\n",
        "multi_tree.fit(X, Y)"
      ],
      "metadata": {
        "id": "1JQkSSiUFB9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code Prediction Evaluation\n",
        "\n",
        "Y_pred = multi_tree.predict(X)\n",
        "\n",
        "print(\"Micro F1:\", f1_score(Y, Y_pred, average='micro'))\n",
        "print(\"Macro F1:\", f1_score(Y, Y_pred, average='macro'))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(Y, Y_pred, target_names=concept_names))\n",
        "\n",
        "for i, tree in enumerate(multi_tree.estimators_):\n",
        "    block_name = concept_names[i]\n",
        "    print(f\"\\n--- Rules for: {block_name} ---\")\n",
        "    print(export_text(tree, feature_names=list(concept_index.keys())))"
      ],
      "metadata": {
        "id": "Fb8z2Rh5KcAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_icd_from_note(note_text):\n",
        "    # Preprocess\n",
        "    structured = preprocess_note(note_text, tokenizer, bert_model)\n",
        "    c_hat = concept_model(structured[\"cls_embedding\"].unsqueeze(0).to(device)).cpu().numpy()\n",
        "\n",
        "    # Predict ICD blocks using symbolic layer\n",
        "    icd_pred = multi_tree.predict(c_hat)\n",
        "    predicted_blocks = [concept_names[i] for i, val in enumerate(icd_pred[0]) if val == 1]\n",
        "\n",
        "    return predicted_blocks"
      ],
      "metadata": {
        "id": "VR8KzIgUKnFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree Visualization\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plot_tree(multi_tree.estimators_[block_index],\n",
        "          feature_names=list(concept_index.keys()),\n",
        "          class_names=[\"Absent\", \"Present\"],\n",
        "          filled=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QNfWDd9qK6IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpretability Metric Evaluation\n",
        "\n",
        "# === 1. Get predictions ===\n",
        "Y_pred = multi_tree.predict(X)\n",
        "\n",
        "# === 2. Compute F1 Scores ===\n",
        "micro_f1 = f1_score(Y_true, Y_pred, average='micro')\n",
        "macro_f1 = f1_score(Y_true, Y_pred, average='macro')\n",
        "\n",
        "print(f\"Micro F1 score: {micro_f1:.4f}\")\n",
        "print(f\"Macro F1 score: {macro_f1:.4f}\")\n",
        "print()\n",
        "\n",
        "# === 3. Avg. Number of Rules per Block ===\n",
        "def count_positive_leaves(tree):\n",
        "    \"\"\"Count number of leaf nodes that predict class 1.\"\"\"\n",
        "    tree_ = tree.tree_\n",
        "    n_nodes = tree_.node_count\n",
        "    children_left = tree_.children_left\n",
        "    children_right = tree_.children_right\n",
        "    value = tree_.value  # shape [node_id, 1, 2]\n",
        "\n",
        "    def is_leaf(node_id):\n",
        "        return children_left[node_id] == children_right[node_id] == -1\n",
        "\n",
        "    count = 0\n",
        "    for node_id in range(n_nodes):\n",
        "        if is_leaf(node_id):\n",
        "            if np.argmax(value[node_id][0]) == 1:  # class 1 (positive)\n",
        "                count += 1\n",
        "    return count\n",
        "\n",
        "total_rules = 0\n",
        "for tree in multi_tree.estimators_:\n",
        "    total_rules += count_positive_leaves(tree)\n",
        "\n",
        "avg_rules_per_block = total_rules / len(multi_tree.estimators_)\n",
        "print(f\"Avg. number of rules per block: {avg_rules_per_block:.2f}\")\n",
        "\n",
        "# === 4. Avg. Concepts per Rule ===\n",
        "def count_concepts_per_rule(tree, feature_names):\n",
        "    tree_ = tree.tree_\n",
        "    paths = []\n",
        "\n",
        "    def recurse(node, path):\n",
        "        if tree_.children_left[node] == -1 and tree_.children_right[node] == -1:\n",
        "            paths.append(path)\n",
        "            return\n",
        "        feat = feature_names[tree_.feature[node]]\n",
        "        recurse(tree_.children_left[node], path + [feat])\n",
        "        recurse(tree_.children_right[node], path + [feat])\n",
        "\n",
        "    recurse(0, [])\n",
        "    concept_counts = [len(set(path)) for path in paths if path]  # avoid empty\n",
        "    return concept_counts\n",
        "\n",
        "all_counts = []\n",
        "feature_names = list(concept_index.keys())\n",
        "for tree in multi_tree.estimators_:\n",
        "    counts = count_concepts_per_rule(tree, feature_names)\n",
        "    all_counts.extend(counts)\n",
        "\n",
        "avg_concepts_per_rule = np.mean(all_counts)\n",
        "print(f\"Avg. concepts used per rule: {avg_concepts_per_rule:.2f}\")\n",
        "\n",
        "# === 5. Optional: Show a few symbolic rules ===\n",
        "print(\"\\n--- Example symbolic rules ---\")\n",
        "for i, tree in enumerate(multi_tree.estimators_[:3]):\n",
        "    print(f\"\\nRules for: {concept_names[i]}\")\n",
        "    print(export_text(tree, feature_names=feature_names))"
      ],
      "metadata": {
        "id": "ye9RTOJAxo-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}